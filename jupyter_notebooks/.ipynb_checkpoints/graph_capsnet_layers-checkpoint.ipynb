{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "'''\n",
    "\n",
    "Much of this code is adapted from code written by \n",
    "Xifeng Guo, E-mail: `guoxifeng1990@163.com`, Github: `https://github.com/XifengGuo/CapsNet-Keras`\n",
    "'''\n",
    "\n",
    "\n",
    "import keras.backend as K\n",
    "import tensorflow as tf\n",
    "from keras import initializers, layers, activations\n",
    "from keras.regularizers import l2\n",
    "\n",
    "\n",
    "class Length(layers.Layer):\n",
    "    \"\"\"\n",
    "    Compute the length of vectors. This is used to compute a Tensor that has the same shape with y_true in margin_loss.\n",
    "    Using this layer as model's output can directly predict labels by using `y_pred = np.argmax(model.predict(x), 1)`\n",
    "    inputs: shape=[None, num_vectors, dim_vector]\n",
    "    output: shape=[None, num_vectors]\n",
    "    \n",
    "    Author: Xifeng Guo, E-mail: `guoxifeng1990@163.com`, Github: `https://github.com/XifengGuo/CapsNet-Keras`\n",
    "    \"\"\"\n",
    "    def call(self, inputs, **kwargs):\n",
    "        return K.sqrt(K.sum(K.square(inputs), -1))\n",
    "\n",
    "    def compute_output_shape(self, input_shape):\n",
    "        return input_shape[:-1]\n",
    "\n",
    "\n",
    "def squash(vectors, axis=-1):\n",
    "    \"\"\"\n",
    "    The non-linear activation used in Capsule. It drives the length of a large vector to near 1 and small vector to 0\n",
    "    :param vectors: some vectors to be squashed, N-dim tensor\n",
    "    :param axis: the axis to squash\n",
    "    :return: a Tensor with same shape as input vectors\n",
    "    \"\"\"\n",
    "    s_squared_norm = K.sum(K.square(vectors), axis, keepdims=True)\n",
    "    scale = s_squared_norm / (1 + s_squared_norm) / K.sqrt(s_squared_norm + K.epsilon())\n",
    "    return scale * vectors\n",
    "\n",
    "class CapsuleLayer(layers.Layer):\n",
    "    \"\"\"\n",
    "    The capsule layer. It is similar to Dense layer. Dense layer has `in_num` inputs, each is a scalar, the output of the \n",
    "    neuron from the former layer, and it has `out_num` output neurons. CapsuleLayer just expand the output of the neuron\n",
    "    from scalar to vector. So its input shape = [None, input_num_capsule, input_dim_capsule] and output shape = \\\n",
    "    [None, num_capsule, dim_capsule]. For Dense Layer, input_dim_capsule = dim_capsule = 1.\n",
    "    \n",
    "    :param num_capsule: number of capsules in this layer\n",
    "    :param dim_capsule: dimension of the output vectors of the capsules in this layer\n",
    "    :param num_routing: number of iterations for the routing algorithm\n",
    "\n",
    "    Author: Xifeng Guo, E-mail: `guoxifeng1990@163.com`, Github: `https://github.com/XifengGuo/CapsNet-Keras`\n",
    "    \"\"\"\n",
    "    def __init__(self, num_capsule, dim_capsule, num_routing=3,\n",
    "                 kernel_initializer='glorot_uniform', kernel_regularizer=l2(1e-2),\n",
    "                 **kwargs):\n",
    "        super(CapsuleLayer, self).__init__(**kwargs)\n",
    "        self.num_capsule = num_capsule\n",
    "        self.dim_capsule = dim_capsule\n",
    "        self.num_routing = num_routing\n",
    "        self.kernel_initializer = initializers.get(kernel_initializer)\n",
    "        self.kernel_regularizer = kernel_regularizer\n",
    "\n",
    "    def build(self, input_shape):\n",
    "        assert len(input_shape) >= 3, \"The input Tensor should have shape=[None, input_num_capsule, input_dim_capsule]\"\n",
    "        self.input_num_capsule = input_shape[1]\n",
    "        self.input_dim_capsule = input_shape[2]\n",
    "\n",
    "        # Transform matrix\n",
    "        self.W = self.add_weight(shape=[self.num_capsule, self.input_num_capsule,\n",
    "                                        self.dim_capsule, self.input_dim_capsule],\n",
    "                                 initializer=self.kernel_initializer,\n",
    "                                 regularizer=self.kernel_regularizer,\n",
    "                                 name='W')\n",
    "\n",
    "        self.built = True\n",
    "\n",
    "    def call(self, inputs, training=None):\n",
    "        # inputs.shape=[None, input_num_capsule, input_dim_capsule]\n",
    "        # inputs_expand.shape=[None, 1, input_num_capsule, input_dim_capsule]\n",
    "        inputs_expand = K.expand_dims(inputs, 1)\n",
    "\n",
    "        # Replicate num_capsule dimension to prepare being multiplied by W\n",
    "        # inputs_tiled.shape=[None, num_capsule, input_num_capsule, input_dim_capsule]\n",
    "        inputs_tiled = K.tile(inputs_expand, [1, self.num_capsule, 1, 1])\n",
    "\n",
    "        # Compute `inputs * W` by scanning inputs_tiled on dimension 0.\n",
    "        # x.shape=[num_capsule, input_num_capsule, input_dim_capsule]\n",
    "        # W.shape=[num_capsule, input_num_capsule, dim_capsule, input_dim_capsule]\n",
    "        # Regard the first two dimensions as `batch` dimension,\n",
    "        # then matmul: [input_dim_capsule] x [dim_capsule, input_dim_capsule]^T -> [dim_capsule].\n",
    "        # inputs_hat.shape = [None, num_capsule, input_num_capsule, dim_capsule]\n",
    "        inputs_hat = K.map_fn(lambda x: K.batch_dot(x, self.W, [2, 3]), elems=inputs_tiled)\n",
    "\n",
    "        \"\"\"\n",
    "        # Begin: routing algorithm V1, dynamic ------------------------------------------------------------#\n",
    "        # The prior for coupling coefficient, initialized as zeros.\n",
    "        b = K.zeros(shape=[self.batch_size, self.num_capsule, self.input_num_capsule])\n",
    "\n",
    "        def body(i, b, outputs):\n",
    "            c = tf.nn.softmax(b, dim=1)  # dim=2 is the num_capsule dimension\n",
    "            outputs = squash(K.batch_dot(c, inputs_hat, [2, 2]))\n",
    "            if i != 1:\n",
    "                b = b + K.batch_dot(outputs, inputs_hat, [2, 3])\n",
    "            return [i-1, b, outputs]\n",
    "\n",
    "        cond = lambda i, b, inputs_hat: i > 0\n",
    "        loop_vars = [K.constant(self.num_routing), b, K.sum(inputs_hat, 2, keepdims=False)]\n",
    "        shape_invariants = [tf.TensorShape([]),\n",
    "                            tf.TensorShape([None, self.num_capsule, self.input_num_capsule]),\n",
    "                            tf.TensorShape([None, self.num_capsule, self.dim_capsule])]\n",
    "        _, _, outputs = tf.while_loop(cond, body, loop_vars, shape_invariants)\n",
    "        # End: routing algorithm V1, dynamic ------------------------------------------------------------#\n",
    "        \"\"\"\n",
    "        # Begin: Routing algorithm ---------------------------------------------------------------------#\n",
    "        # In forward pass, `inputs_hat_stopped` = `inputs_hat`;\n",
    "        # In backward, no gradient can flow from `inputs_hat_stopped` back to `inputs_hat`.\n",
    "        inputs_hat_stopped = K.stop_gradient(inputs_hat)\n",
    "        \n",
    "        # The prior for coupling coefficient, initialized as zeros.\n",
    "        # b.shape = [None, self.num_capsule, self.input_num_capsule].\n",
    "        b = tf.zeros(shape=[K.shape(inputs_hat)[0], self.num_capsule, self.input_num_capsule])\n",
    "\n",
    "        assert self.num_routing > 0, 'The num_routing should be > 0.'\n",
    "        for i in range(self.num_routing):\n",
    "            # c.shape=[batch_size, num_capsule, input_num_capsule]\n",
    "            c = tf.nn.softmax(b, dim=1)\n",
    "\n",
    "            # At last iteration, use `inputs_hat` to compute `outputs` in order to backpropagate gradient\n",
    "            if i == self.num_routing - 1:\n",
    "                # c.shape =  [batch_size, num_capsule, input_num_capsule]\n",
    "                # inputs_hat.shape=[None, num_capsule, input_num_capsule, dim_capsule]\n",
    "                # The first two dimensions as `batch` dimension,\n",
    "                # then matmal: [input_num_capsule] x [input_num_capsule, dim_capsule] -> [dim_capsule].\n",
    "                # outputs.shape=[None, num_capsule, dim_capsule]\n",
    "                outputs = squash(K.batch_dot(c, inputs_hat, [2, 2]))  # [None, 10, 16]\n",
    "            else:  # Otherwise, use `inputs_hat_stopped` to update `b`. No gradients flow on this path.\n",
    "                outputs = squash(K.batch_dot(c, inputs_hat_stopped, [2, 2]))\n",
    "\n",
    "                # outputs.shape =  [None, num_capsule, dim_capsule]\n",
    "                # inputs_hat.shape=[None, num_capsule, input_num_capsule, dim_capsule]\n",
    "                # The first two dimensions as `batch` dimension,\n",
    "                # then matmal: [dim_capsule] x [input_num_capsule, dim_capsule]^T -> [input_num_capsule].\n",
    "                # b.shape=[batch_size, num_capsule, input_num_capsule]\n",
    "                b += K.batch_dot(outputs, inputs_hat_stopped, [2, 3])\n",
    "        # End: Routing algorithm -----------------------------------------------------------------------#\n",
    "\n",
    "        return outputs\n",
    "\n",
    "    def compute_output_shape(self, input_shape):\n",
    "        return tuple([None, self.num_capsule, self.dim_capsule])\n",
    "\n",
    "\n",
    "def PrimaryCap(x, L, dim_capsule, num_capsule, kernel_size, activation=None):\n",
    "    \"\"\"\n",
    "    Apply GraphConv `n_channels` times and concatenate all capsules\n",
    "    :param inputs: 3D tensor, shape=[None, N, channels]\n",
    "    :param dim_capsule: the dim of the output vector of capsule\n",
    "    :param n_channels: the number of types of capsules\n",
    "    :return: output tensor, shape=[None, num_capsule, dim_capsule]\n",
    "\n",
    "    Author: David McDonald, E-mail: `dxm237@cs.bham.ac.uk`\n",
    "    \"\"\"\n",
    "    output = GraphConv(num_filters=dim_capsule*num_capsule, \n",
    "                       kernel_size=kernel_size, activation=activation,\n",
    "                           name='primarycap_GraphConv')([L, x])\n",
    "    outputs = layers.Reshape(target_shape=[-1, dim_capsule], name='primarycap_reshape')(output)\n",
    "    return layers.Lambda(squash, name='primarycap_squash')(outputs)\n",
    "\n",
    "class GraphConv(layers.Layer):\n",
    "    \"\"\"\n",
    "    A layer to perform a convolutional filter on a graph\n",
    "\n",
    "    :param L_hat: rescaled graph laplacian: L_hat = 2L/lambda_max - identity(N)\n",
    "    :param num_filters: number of filters\n",
    "    :param kernel_size: size of kernel to use\n",
    "    \n",
    "    Author: David McDonald, Email: `dxm237@cs.bham.ac.uk'\n",
    "    \"\"\"\n",
    "    def __init__(self, num_filters, kernel_size, activation=None,\n",
    "                 kernel_initializer='glorot_uniform', kernel_regularizer=l2(1e-2),\n",
    "                 **kwargs):\n",
    "        super(GraphConv, self).__init__(**kwargs)\n",
    "        # SparseTensor\n",
    "#         self.L_hat = L_hat\n",
    "        self.num_filters = num_filters\n",
    "        self.kernel_size = kernel_size\n",
    "        self.activation = activation\n",
    "        self.kernel_initializer = initializers.get(kernel_initializer)\n",
    "        self.kernel_regularizer = kernel_regularizer\n",
    "\n",
    "    def build(self, input_shape):\n",
    "        '''\n",
    "        L input shape[0] = [batch_size, batch_size]\n",
    "        x input_shape[1] = [batch_size, D, input_channels]\n",
    "        '''\n",
    "        \n",
    "        _, D, num_channels = input_shape[1]\n",
    "        \n",
    "        # chebyshev co-efficients\n",
    "        self.W = self.add_weight(shape=[num_channels * (self.kernel_size + 1), self.num_filters],\n",
    "                                 initializer=self.kernel_initializer, regularizer=self.kernel_regularizer,\n",
    "                                 name='chebyshev_co-efficients')\n",
    "        \n",
    "        # bias term ?\n",
    "        self.bias = self.add_weight(shape=[1, D, self.num_filters], \n",
    "                                    initializer=self.kernel_initializer, regularizer=self.kernel_regularizer,\n",
    "                                   name=\"bias\")\n",
    "\n",
    "        self.built = True\n",
    "        \n",
    "    def sparse_matmul(self, L, x):\n",
    "        '''\n",
    "        L.shape = [N, N]\n",
    "        x.shape = [D, N, C]\n",
    "        \n",
    "        permute dimensions of x to [N, D, C]\n",
    "        reshape to [N, D*C]\n",
    "        sparse to dense matmul with L: [N, N] @ [N, D*C] -> [N, D*C]\n",
    "        reshape to [N, D, C]\n",
    "        permute axes to [D, N, C]\n",
    "        \n",
    "        ----------------------------------------------------\n",
    "        \n",
    "        L.shape = [N, N]\n",
    "        x.shape = [N, D, C]\n",
    "        \n",
    "        reshape to [N, D*C]\n",
    "        sparse to dense matmul with L: [N, N] @ [N, D*C] -> [N, D*C]\n",
    "        reshape to [N, D, C]\n",
    "        \n",
    "        '''\n",
    "        N = K.shape(x)[0]\n",
    "        D = K.shape(x)[1]\n",
    "        C = K.shape(x)[2]\n",
    "        \n",
    "        x_transformed = x\n",
    "#         x_transformed = K.permute_dimensions(x_transformed, [1,0,2])\n",
    "        x_transformed = K.reshape(x_transformed, [N, D*C])\n",
    "        x_transformed = tf.matmul(L, x_transformed)\n",
    "#         x_transformed =  K.reshape(x_transformed, [N, D, C])\n",
    "#         return K.permute_dimensions(x_transformed, [1, 0, 2])\n",
    "        return K.reshape(x_transformed, [N, D, C])\n",
    "        \n",
    "    def compute_x_hat(self, L, x):\n",
    "        '''\n",
    "        efficient computation of filter using recurrence relation of Chebyshev polynomials\n",
    "        T_k(x) = 2xT_k-1(x) - T_k-2(x)\n",
    "        T_0(x) = 1\n",
    "        T_1(x) = x\n",
    "        \n",
    "        input shape is [None, D, input_channels]\n",
    "        output shape is [None, D, filters]\n",
    "        \n",
    "        '''\n",
    "\n",
    "        x_hat = [x]\n",
    "        \n",
    "        if self.kernel_size > 0:\n",
    "            x_hat.append(self.sparse_matmul(L, x))\n",
    "        \n",
    "        for k in range(self.kernel_size - 1):\n",
    "            x_hat.append(2 * self.sparse_matmul(L, x_hat[-1]) - x_hat[-2])\n",
    "    \n",
    "        # concatenate to combine input_channels and kernel_size axes\n",
    "        return K.concatenate(x_hat, axis=-1)\n",
    "        \n",
    "\n",
    "    def call(self, inputs):\n",
    "\n",
    "        '''\n",
    "        chebyshev polynomial for filtering using coefficients started in kernel\n",
    "        '''\n",
    "        L, x = inputs\n",
    "        x_hat = self.compute_x_hat(L, x)\n",
    "        # x_hat shape = [None, D, input_channels*filter_size]\n",
    "        # W shape = [input_channels*filter_size, num_filters]\n",
    "        # output shape = [None, D, num_filters]\n",
    "        output = K.dot(x_hat, self.W)\n",
    "        \n",
    "#         print output.shape\n",
    "#         print self.bias.shape\n",
    "        \n",
    "        output += self.bias\n",
    "\n",
    "        if self.activation is not None:\n",
    "            output = activations.get(self.activation)(output)\n",
    "        \n",
    "        return output\n",
    "        \n",
    "    def compute_output_shape(self, input_shape):\n",
    "        '''\n",
    "        L input shape[0] = [batch_size, batch_size]\n",
    "        x input_shape[1] = [batch_size, D, input_channels]\n",
    "        \n",
    "        output_shape is [None, D, num_filters]\n",
    "        '''\n",
    "        return tuple([None, input_shape[1][1], self.num_filters])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def masked_crossentropy(y_true, y_pred):\n",
    "    '''\n",
    "    y_true shape = [None, num_classes]\n",
    "    y_pred shape = [None, 2*  num_classes]\n",
    "    \n",
    "    '''    \n",
    "    mask, y_pred = tf.split(y_pred, num_or_size_splits=2, axis=-1)\n",
    "    \n",
    "    y_pred = K.clip(y_pred, min_value=K.epsilon(), max_value=1-K.epsilon())\n",
    "    \n",
    "    return -K.sum(mask * y_true * K.log(y_pred))\n",
    "\n",
    "def masked_margin_loss(y_true, y_pred):\n",
    "    \"\"\"\n",
    "    Margin loss for Eq.(4). When y_true[i, :] contains not just one `1`, this loss should work too. Not test it.\n",
    "    :param y_true: [None, n_classes]\n",
    "    :param y_pred: [None, 2 * num_classes]\n",
    "    :return: a scalar loss value.\n",
    "    \"\"\"\n",
    "\n",
    "    mask, y_pred = tf.split(y_pred, num_or_size_splits=2, axis=-1)\n",
    "    \n",
    "    L = y_true * K.square(K.maximum(0., 0.9 - y_pred)) + \\\n",
    "        0.5 * (1 - y_true) * K.square(K.maximum(0., y_pred - 0.1))\n",
    "        \n",
    "    L *= mask\n",
    "\n",
    "    return K.sum(K.sum(L, 1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import scipy as sp\n",
    "import pandas as pd\n",
    "\n",
    "from keras.models import Model\n",
    "from keras.layers import Input\n",
    "\n",
    "import networkx as nx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"../data/cora/cites.tsv\", \"rb\") as inf:\n",
    "    next(inf, \"\")\n",
    "    G = nx.read_edgelist(inf, delimiter=\"\\t\", )\n",
    "\n",
    "X = sp.sparse.load_npz(\"../data/cora/cited_words.npz\")\n",
    "Y = sp.sparse.load_npz(\"../data/cora/paper_labels.npz\")\n",
    "\n",
    "N, D = X.shape\n",
    "_, num_classes = Y.shape\n",
    "C = 1\n",
    "\n",
    "X = np.reshape(X.toarray(), [N, D, C]).astype(np.float32)\n",
    "Y = Y.toarray().astype(np.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# G = nx.karate_club_graph()\n",
    "# N = nx.number_of_nodes(G)\n",
    "\n",
    "# X = np.identity(N)\n",
    "# map_ = {\"Mr. Hi\" : 0, \"Officer\" : 1}\n",
    "# Y = np.zeros((N, 2))\n",
    "# for i, v in enumerate(nx.get_node_attributes(G, \"club\").values()):\n",
    "#     Y[i, map_[v]] = 1\n",
    "    \n",
    "# N, D = X.shape\n",
    "# _, num_classes = Y.shape\n",
    "# C = 1\n",
    "\n",
    "# X = np.reshape(X, [N, D, C])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "p = 20\n",
    "assignments = Y.argmax(axis=1)\n",
    "patterns_to_remove = np.concatenate([np.random.permutation(np.where(assignments==i)[0])[:-p] \n",
    "                                     for i in range(num_classes)])\n",
    "mask = np.ones(Y.shape, dtype=np.float32)\n",
    "mask[patterns_to_remove] = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "L = nx.laplacian_matrix(G).astype(np.float32)\n",
    "l, U = sp.sparse.linalg.eigen.arpack.eigsh(L, k=1, which=\"LM\")\n",
    "\n",
    "L_hat = (2 * L / l[0] - sp.sparse.identity(L.shape[0], dtype=np.float32)).astype(np.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sparse_matrix_power(M, n):\n",
    "    if n==0:\n",
    "        return sp.sparse.identity(M.shape[0])\n",
    "    if n%2==0:\n",
    "        x = sparse_matrix_power(M, n/2)\n",
    "        return x.dot(x)\n",
    "    return M.dot(sparse_matrix_power(M, n-1))\n",
    "\n",
    "def pad_neighbours(N, neighbours, n):\n",
    "    num_neighbours = len(neighbours)\n",
    "    if num_neighbours < n:\n",
    "        non_neighbours = np.arange(N)[np.isin(np.arange(N), neighbours, assume_unique=True, invert=True)]\n",
    "        neighbours = np.append(neighbours, np.random.choice(non_neighbours, replace=False, size=(n-num_neighbours)))\n",
    "    return neighbours\n",
    "\n",
    "def n_hop_neighbours(L, batch_size=100, n=2):\n",
    "    N = L.shape[0]\n",
    "    I, J = np.nonzero(sparse_matrix_power(L, n))\n",
    "    return [pad_neighbours(N, J[I==i], batch_size) for i in range(N)]\n",
    "\n",
    "def generator(L, X, Y, mask, num_nodes=1, num_neighbours=100, n=2):\n",
    "    n_hop_nodes = n_hop_neighbours(L, batch_size=batch_size, n=n)\n",
    "    N = L.shape[0]\n",
    "    i = 0\n",
    "    nodes = np.random.permutation(N)\n",
    "    batch = np.zeros(num_nodes*num_neighbours, dtype=np.int)\n",
    "    while True:\n",
    "        \n",
    "        for node in range(num_nodes):\n",
    "            if i == N:\n",
    "                i -= N\n",
    "                nodes = np.random.permutation(N)\n",
    "            n = nodes[i]\n",
    "            neighbours = np.random.choice(n_hop_nodes[n], size=num_neighbours-1, replace=False )\n",
    "            batch[node*num_neighbours] = n\n",
    "            batch[node*num_neighbours+1:(node+1)*num_neighbours] = neighbours\n",
    "            i += 1\n",
    "        if (mask[batch]>0).any():\n",
    "            yield [X[batch], L[batch][:, batch].todense(), mask[batch]], Y[batch]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "num_nodes = 2\n",
    "num_neighbours = 25\n",
    "batch_size = num_nodes * num_neighbours"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = Input(shape=(D, C), name=\"signal_input\")\n",
    "L_input = Input(shape=(batch_size,),  name=\"L_input\")\n",
    "mask_input = Input(shape=(num_classes,), name=\"mask_input\")\n",
    "\n",
    "conv1 = GraphConv(num_filters=32, kernel_size=1, activation=\"relu\", \n",
    "                  name=\"conv1\")([L_input, x])\n",
    "primary_cap = PrimaryCap(x=conv1, L=L_input, \n",
    "                         dim_capsule=8, num_capsule=32, kernel_size=1)\n",
    "secondary_cap = CapsuleLayer(dim_capsule=16, num_capsule=num_classes, \n",
    "                             num_routing=3, name=\"secondary_cap\")(primary_cap)\n",
    "secondary_cap_length = Length(name=\"length\")(secondary_cap)\n",
    "\n",
    "outputs = layers.Concatenate()([mask_input, secondary_cap_length])\n",
    "\n",
    "model = Model([x, L_input, mask_input], [outputs])\n",
    "\n",
    "model.compile(optimizer=\"adam\", #loss=[masked_margin_loss])\n",
    "              loss=masked_crossentropy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "L_input (InputLayer)            (None, 50)           0                                            \n",
      "__________________________________________________________________________________________________\n",
      "signal_input (InputLayer)       (None, 1433, 1)      0                                            \n",
      "__________________________________________________________________________________________________\n",
      "conv1 (GraphConv)               (None, 1433, 32)     45920       L_input[0][0]                    \n",
      "                                                                 signal_input[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "primarycap_GraphConv (GraphConv (None, 1433, 256)    383232      L_input[0][0]                    \n",
      "                                                                 conv1[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "primarycap_reshape (Reshape)    (None, 45856, 8)     0           primarycap_GraphConv[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "primarycap_squash (Lambda)      (None, 45856, 8)     0           primarycap_reshape[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "secondary_cap (CapsuleLayer)    (None, 7, 16)        41086976    primarycap_squash[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "mask_input (InputLayer)         (None, 7)            0                                            \n",
      "__________________________________________________________________________________________________\n",
      "length (Length)                 (None, 7)            0           secondary_cap[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_1 (Concatenate)     (None, 14)           0           mask_input[0][0]                 \n",
      "                                                                 length[0][0]                     \n",
      "==================================================================================================\n",
      "Total params: 41,516,128\n",
      "Trainable params: 41,516,128\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from keras.callbacks import TerminateOnNaN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "gen = generator(L_hat, X, Y, mask, num_nodes=num_nodes, num_neighbours=num_neighbours, n=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/1\n",
      " 155/1354 [==>...........................] - ETA: 54:49 - loss: 40.9714Batch 155: Invalid loss, terminating training\n",
      " 156/1354 [==>...........................] - ETA: 54:46 - loss: nan   "
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7fbf3c46f550>"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit_generator(generator=gen,\n",
    "                    epochs=1, steps_per_epoch=N/2, callbacks=[TerminateOnNaN()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[]\n",
      "[]\n",
      "[array([[ nan,  nan,  nan,  nan,  nan,  nan,  nan,  nan,  nan,  nan,  nan,\n",
      "         nan,  nan,  nan,  nan,  nan,  nan,  nan,  nan,  nan,  nan,  nan,\n",
      "         nan,  nan,  nan,  nan,  nan,  nan,  nan,  nan,  nan,  nan],\n",
      "       [ nan,  nan,  nan,  nan,  nan,  nan,  nan,  nan,  nan,  nan,  nan,\n",
      "         nan,  nan,  nan,  nan,  nan,  nan,  nan,  nan,  nan,  nan,  nan,\n",
      "         nan,  nan,  nan,  nan,  nan,  nan,  nan,  nan,  nan,  nan]], dtype=float32), array([[[        nan,         nan,         nan, ...,         nan,\n",
      "                 nan,         nan],\n",
      "        [        nan,         nan,         nan, ...,         nan,\n",
      "                 nan,         nan],\n",
      "        [        nan,         nan,         nan, ...,         nan,\n",
      "                 nan, -0.00012895],\n",
      "        ..., \n",
      "        [        nan,         nan,         nan, ...,         nan,\n",
      "         -0.00286441,         nan],\n",
      "        [        nan,         nan,         nan, ...,         nan,\n",
      "                 nan, -0.00073145],\n",
      "        [        nan,         nan,         nan, ...,         nan,\n",
      "                 nan,         nan]]], dtype=float32)]\n",
      "[array([[ nan,  nan,  nan, ...,  nan,  nan,  nan],\n",
      "       [ nan,  nan,  nan, ...,  nan,  nan,  nan],\n",
      "       [ nan,  nan,  nan, ...,  nan,  nan,  nan],\n",
      "       ..., \n",
      "       [ nan,  nan,  nan, ...,  nan,  nan,  nan],\n",
      "       [ nan,  nan,  nan, ...,  nan,  nan,  nan],\n",
      "       [ nan,  nan,  nan, ...,  nan,  nan,  nan]], dtype=float32), array([[[ nan,  nan,  nan, ...,  nan,  nan,  nan],\n",
      "        [ nan,  nan,  nan, ...,  nan,  nan,  nan],\n",
      "        [ nan,  nan,  nan, ...,  nan,  nan,  nan],\n",
      "        ..., \n",
      "        [ nan,  nan,  nan, ...,  nan,  nan,  nan],\n",
      "        [ nan,  nan,  nan, ...,  nan,  nan,  nan],\n",
      "        [ nan,  nan,  nan, ...,  nan,  nan,  nan]]], dtype=float32)]\n",
      "[]\n",
      "[]\n",
      "[array([[[[ nan,  nan,  nan, ...,  nan,  nan,  nan],\n",
      "         [ nan,  nan,  nan, ...,  nan,  nan,  nan],\n",
      "         [ nan,  nan,  nan, ...,  nan,  nan,  nan],\n",
      "         ..., \n",
      "         [ nan,  nan,  nan, ...,  nan,  nan,  nan],\n",
      "         [ nan,  nan,  nan, ...,  nan,  nan,  nan],\n",
      "         [ nan,  nan,  nan, ...,  nan,  nan,  nan]],\n",
      "\n",
      "        [[ nan,  nan,  nan, ...,  nan,  nan,  nan],\n",
      "         [ nan,  nan,  nan, ...,  nan,  nan,  nan],\n",
      "         [ nan,  nan,  nan, ...,  nan,  nan,  nan],\n",
      "         ..., \n",
      "         [ nan,  nan,  nan, ...,  nan,  nan,  nan],\n",
      "         [ nan,  nan,  nan, ...,  nan,  nan,  nan],\n",
      "         [ nan,  nan,  nan, ...,  nan,  nan,  nan]],\n",
      "\n",
      "        [[ nan,  nan,  nan, ...,  nan,  nan,  nan],\n",
      "         [ nan,  nan,  nan, ...,  nan,  nan,  nan],\n",
      "         [ nan,  nan,  nan, ...,  nan,  nan,  nan],\n",
      "         ..., \n",
      "         [ nan,  nan,  nan, ...,  nan,  nan,  nan],\n",
      "         [ nan,  nan,  nan, ...,  nan,  nan,  nan],\n",
      "         [ nan,  nan,  nan, ...,  nan,  nan,  nan]],\n",
      "\n",
      "        ..., \n",
      "        [[ nan,  nan,  nan, ...,  nan,  nan,  nan],\n",
      "         [ nan,  nan,  nan, ...,  nan,  nan,  nan],\n",
      "         [ nan,  nan,  nan, ...,  nan,  nan,  nan],\n",
      "         ..., \n",
      "         [ nan,  nan,  nan, ...,  nan,  nan,  nan],\n",
      "         [ nan,  nan,  nan, ...,  nan,  nan,  nan],\n",
      "         [ nan,  nan,  nan, ...,  nan,  nan,  nan]],\n",
      "\n",
      "        [[ nan,  nan,  nan, ...,  nan,  nan,  nan],\n",
      "         [ nan,  nan,  nan, ...,  nan,  nan,  nan],\n",
      "         [ nan,  nan,  nan, ...,  nan,  nan,  nan],\n",
      "         ..., \n",
      "         [ nan,  nan,  nan, ...,  nan,  nan,  nan],\n",
      "         [ nan,  nan,  nan, ...,  nan,  nan,  nan],\n",
      "         [ nan,  nan,  nan, ...,  nan,  nan,  nan]],\n",
      "\n",
      "        [[ nan,  nan,  nan, ...,  nan,  nan,  nan],\n",
      "         [ nan,  nan,  nan, ...,  nan,  nan,  nan],\n",
      "         [ nan,  nan,  nan, ...,  nan,  nan,  nan],\n",
      "         ..., \n",
      "         [ nan,  nan,  nan, ...,  nan,  nan,  nan],\n",
      "         [ nan,  nan,  nan, ...,  nan,  nan,  nan],\n",
      "         [ nan,  nan,  nan, ...,  nan,  nan,  nan]]],\n",
      "\n",
      "\n",
      "       [[[ nan,  nan,  nan, ...,  nan,  nan,  nan],\n",
      "         [ nan,  nan,  nan, ...,  nan,  nan,  nan],\n",
      "         [ nan,  nan,  nan, ...,  nan,  nan,  nan],\n",
      "         ..., \n",
      "         [ nan,  nan,  nan, ...,  nan,  nan,  nan],\n",
      "         [ nan,  nan,  nan, ...,  nan,  nan,  nan],\n",
      "         [ nan,  nan,  nan, ...,  nan,  nan,  nan]],\n",
      "\n",
      "        [[ nan,  nan,  nan, ...,  nan,  nan,  nan],\n",
      "         [ nan,  nan,  nan, ...,  nan,  nan,  nan],\n",
      "         [ nan,  nan,  nan, ...,  nan,  nan,  nan],\n",
      "         ..., \n",
      "         [ nan,  nan,  nan, ...,  nan,  nan,  nan],\n",
      "         [ nan,  nan,  nan, ...,  nan,  nan,  nan],\n",
      "         [ nan,  nan,  nan, ...,  nan,  nan,  nan]],\n",
      "\n",
      "        [[ nan,  nan,  nan, ...,  nan,  nan,  nan],\n",
      "         [ nan,  nan,  nan, ...,  nan,  nan,  nan],\n",
      "         [ nan,  nan,  nan, ...,  nan,  nan,  nan],\n",
      "         ..., \n",
      "         [ nan,  nan,  nan, ...,  nan,  nan,  nan],\n",
      "         [ nan,  nan,  nan, ...,  nan,  nan,  nan],\n",
      "         [ nan,  nan,  nan, ...,  nan,  nan,  nan]],\n",
      "\n",
      "        ..., \n",
      "        [[ nan,  nan,  nan, ...,  nan,  nan,  nan],\n",
      "         [ nan,  nan,  nan, ...,  nan,  nan,  nan],\n",
      "         [ nan,  nan,  nan, ...,  nan,  nan,  nan],\n",
      "         ..., \n",
      "         [ nan,  nan,  nan, ...,  nan,  nan,  nan],\n",
      "         [ nan,  nan,  nan, ...,  nan,  nan,  nan],\n",
      "         [ nan,  nan,  nan, ...,  nan,  nan,  nan]],\n",
      "\n",
      "        [[ nan,  nan,  nan, ...,  nan,  nan,  nan],\n",
      "         [ nan,  nan,  nan, ...,  nan,  nan,  nan],\n",
      "         [ nan,  nan,  nan, ...,  nan,  nan,  nan],\n",
      "         ..., \n",
      "         [ nan,  nan,  nan, ...,  nan,  nan,  nan],\n",
      "         [ nan,  nan,  nan, ...,  nan,  nan,  nan],\n",
      "         [ nan,  nan,  nan, ...,  nan,  nan,  nan]],\n",
      "\n",
      "        [[ nan,  nan,  nan, ...,  nan,  nan,  nan],\n",
      "         [ nan,  nan,  nan, ...,  nan,  nan,  nan],\n",
      "         [ nan,  nan,  nan, ...,  nan,  nan,  nan],\n",
      "         ..., \n",
      "         [ nan,  nan,  nan, ...,  nan,  nan,  nan],\n",
      "         [ nan,  nan,  nan, ...,  nan,  nan,  nan],\n",
      "         [ nan,  nan,  nan, ...,  nan,  nan,  nan]]],\n",
      "\n",
      "\n",
      "       [[[ nan,  nan,  nan, ...,  nan,  nan,  nan],\n",
      "         [ nan,  nan,  nan, ...,  nan,  nan,  nan],\n",
      "         [ nan,  nan,  nan, ...,  nan,  nan,  nan],\n",
      "         ..., \n",
      "         [ nan,  nan,  nan, ...,  nan,  nan,  nan],\n",
      "         [ nan,  nan,  nan, ...,  nan,  nan,  nan],\n",
      "         [ nan,  nan,  nan, ...,  nan,  nan,  nan]],\n",
      "\n",
      "        [[ nan,  nan,  nan, ...,  nan,  nan,  nan],\n",
      "         [ nan,  nan,  nan, ...,  nan,  nan,  nan],\n",
      "         [ nan,  nan,  nan, ...,  nan,  nan,  nan],\n",
      "         ..., \n",
      "         [ nan,  nan,  nan, ...,  nan,  nan,  nan],\n",
      "         [ nan,  nan,  nan, ...,  nan,  nan,  nan],\n",
      "         [ nan,  nan,  nan, ...,  nan,  nan,  nan]],\n",
      "\n",
      "        [[ nan,  nan,  nan, ...,  nan,  nan,  nan],\n",
      "         [ nan,  nan,  nan, ...,  nan,  nan,  nan],\n",
      "         [ nan,  nan,  nan, ...,  nan,  nan,  nan],\n",
      "         ..., \n",
      "         [ nan,  nan,  nan, ...,  nan,  nan,  nan],\n",
      "         [ nan,  nan,  nan, ...,  nan,  nan,  nan],\n",
      "         [ nan,  nan,  nan, ...,  nan,  nan,  nan]],\n",
      "\n",
      "        ..., \n",
      "        [[ nan,  nan,  nan, ...,  nan,  nan,  nan],\n",
      "         [ nan,  nan,  nan, ...,  nan,  nan,  nan],\n",
      "         [ nan,  nan,  nan, ...,  nan,  nan,  nan],\n",
      "         ..., \n",
      "         [ nan,  nan,  nan, ...,  nan,  nan,  nan],\n",
      "         [ nan,  nan,  nan, ...,  nan,  nan,  nan],\n",
      "         [ nan,  nan,  nan, ...,  nan,  nan,  nan]],\n",
      "\n",
      "        [[ nan,  nan,  nan, ...,  nan,  nan,  nan],\n",
      "         [ nan,  nan,  nan, ...,  nan,  nan,  nan],\n",
      "         [ nan,  nan,  nan, ...,  nan,  nan,  nan],\n",
      "         ..., \n",
      "         [ nan,  nan,  nan, ...,  nan,  nan,  nan],\n",
      "         [ nan,  nan,  nan, ...,  nan,  nan,  nan],\n",
      "         [ nan,  nan,  nan, ...,  nan,  nan,  nan]],\n",
      "\n",
      "        [[ nan,  nan,  nan, ...,  nan,  nan,  nan],\n",
      "         [ nan,  nan,  nan, ...,  nan,  nan,  nan],\n",
      "         [ nan,  nan,  nan, ...,  nan,  nan,  nan],\n",
      "         ..., \n",
      "         [ nan,  nan,  nan, ...,  nan,  nan,  nan],\n",
      "         [ nan,  nan,  nan, ...,  nan,  nan,  nan],\n",
      "         [ nan,  nan,  nan, ...,  nan,  nan,  nan]]],\n",
      "\n",
      "\n",
      "       ..., \n",
      "       [[[ nan,  nan,  nan, ...,  nan,  nan,  nan],\n",
      "         [ nan,  nan,  nan, ...,  nan,  nan,  nan],\n",
      "         [ nan,  nan,  nan, ...,  nan,  nan,  nan],\n",
      "         ..., \n",
      "         [ nan,  nan,  nan, ...,  nan,  nan,  nan],\n",
      "         [ nan,  nan,  nan, ...,  nan,  nan,  nan],\n",
      "         [ nan,  nan,  nan, ...,  nan,  nan,  nan]],\n",
      "\n",
      "        [[ nan,  nan,  nan, ...,  nan,  nan,  nan],\n",
      "         [ nan,  nan,  nan, ...,  nan,  nan,  nan],\n",
      "         [ nan,  nan,  nan, ...,  nan,  nan,  nan],\n",
      "         ..., \n",
      "         [ nan,  nan,  nan, ...,  nan,  nan,  nan],\n",
      "         [ nan,  nan,  nan, ...,  nan,  nan,  nan],\n",
      "         [ nan,  nan,  nan, ...,  nan,  nan,  nan]],\n",
      "\n",
      "        [[ nan,  nan,  nan, ...,  nan,  nan,  nan],\n",
      "         [ nan,  nan,  nan, ...,  nan,  nan,  nan],\n",
      "         [ nan,  nan,  nan, ...,  nan,  nan,  nan],\n",
      "         ..., \n",
      "         [ nan,  nan,  nan, ...,  nan,  nan,  nan],\n",
      "         [ nan,  nan,  nan, ...,  nan,  nan,  nan],\n",
      "         [ nan,  nan,  nan, ...,  nan,  nan,  nan]],\n",
      "\n",
      "        ..., \n",
      "        [[ nan,  nan,  nan, ...,  nan,  nan,  nan],\n",
      "         [ nan,  nan,  nan, ...,  nan,  nan,  nan],\n",
      "         [ nan,  nan,  nan, ...,  nan,  nan,  nan],\n",
      "         ..., \n",
      "         [ nan,  nan,  nan, ...,  nan,  nan,  nan],\n",
      "         [ nan,  nan,  nan, ...,  nan,  nan,  nan],\n",
      "         [ nan,  nan,  nan, ...,  nan,  nan,  nan]],\n",
      "\n",
      "        [[ nan,  nan,  nan, ...,  nan,  nan,  nan],\n",
      "         [ nan,  nan,  nan, ...,  nan,  nan,  nan],\n",
      "         [ nan,  nan,  nan, ...,  nan,  nan,  nan],\n",
      "         ..., \n",
      "         [ nan,  nan,  nan, ...,  nan,  nan,  nan],\n",
      "         [ nan,  nan,  nan, ...,  nan,  nan,  nan],\n",
      "         [ nan,  nan,  nan, ...,  nan,  nan,  nan]],\n",
      "\n",
      "        [[ nan,  nan,  nan, ...,  nan,  nan,  nan],\n",
      "         [ nan,  nan,  nan, ...,  nan,  nan,  nan],\n",
      "         [ nan,  nan,  nan, ...,  nan,  nan,  nan],\n",
      "         ..., \n",
      "         [ nan,  nan,  nan, ...,  nan,  nan,  nan],\n",
      "         [ nan,  nan,  nan, ...,  nan,  nan,  nan],\n",
      "         [ nan,  nan,  nan, ...,  nan,  nan,  nan]]],\n",
      "\n",
      "\n",
      "       [[[ nan,  nan,  nan, ...,  nan,  nan,  nan],\n",
      "         [ nan,  nan,  nan, ...,  nan,  nan,  nan],\n",
      "         [ nan,  nan,  nan, ...,  nan,  nan,  nan],\n",
      "         ..., \n",
      "         [ nan,  nan,  nan, ...,  nan,  nan,  nan],\n",
      "         [ nan,  nan,  nan, ...,  nan,  nan,  nan],\n",
      "         [ nan,  nan,  nan, ...,  nan,  nan,  nan]],\n",
      "\n",
      "        [[ nan,  nan,  nan, ...,  nan,  nan,  nan],\n",
      "         [ nan,  nan,  nan, ...,  nan,  nan,  nan],\n",
      "         [ nan,  nan,  nan, ...,  nan,  nan,  nan],\n",
      "         ..., \n",
      "         [ nan,  nan,  nan, ...,  nan,  nan,  nan],\n",
      "         [ nan,  nan,  nan, ...,  nan,  nan,  nan],\n",
      "         [ nan,  nan,  nan, ...,  nan,  nan,  nan]],\n",
      "\n",
      "        [[ nan,  nan,  nan, ...,  nan,  nan,  nan],\n",
      "         [ nan,  nan,  nan, ...,  nan,  nan,  nan],\n",
      "         [ nan,  nan,  nan, ...,  nan,  nan,  nan],\n",
      "         ..., \n",
      "         [ nan,  nan,  nan, ...,  nan,  nan,  nan],\n",
      "         [ nan,  nan,  nan, ...,  nan,  nan,  nan],\n",
      "         [ nan,  nan,  nan, ...,  nan,  nan,  nan]],\n",
      "\n",
      "        ..., \n",
      "        [[ nan,  nan,  nan, ...,  nan,  nan,  nan],\n",
      "         [ nan,  nan,  nan, ...,  nan,  nan,  nan],\n",
      "         [ nan,  nan,  nan, ...,  nan,  nan,  nan],\n",
      "         ..., \n",
      "         [ nan,  nan,  nan, ...,  nan,  nan,  nan],\n",
      "         [ nan,  nan,  nan, ...,  nan,  nan,  nan],\n",
      "         [ nan,  nan,  nan, ...,  nan,  nan,  nan]],\n",
      "\n",
      "        [[ nan,  nan,  nan, ...,  nan,  nan,  nan],\n",
      "         [ nan,  nan,  nan, ...,  nan,  nan,  nan],\n",
      "         [ nan,  nan,  nan, ...,  nan,  nan,  nan],\n",
      "         ..., \n",
      "         [ nan,  nan,  nan, ...,  nan,  nan,  nan],\n",
      "         [ nan,  nan,  nan, ...,  nan,  nan,  nan],\n",
      "         [ nan,  nan,  nan, ...,  nan,  nan,  nan]],\n",
      "\n",
      "        [[ nan,  nan,  nan, ...,  nan,  nan,  nan],\n",
      "         [ nan,  nan,  nan, ...,  nan,  nan,  nan],\n",
      "         [ nan,  nan,  nan, ...,  nan,  nan,  nan],\n",
      "         ..., \n",
      "         [ nan,  nan,  nan, ...,  nan,  nan,  nan],\n",
      "         [ nan,  nan,  nan, ...,  nan,  nan,  nan],\n",
      "         [ nan,  nan,  nan, ...,  nan,  nan,  nan]]],\n",
      "\n",
      "\n",
      "       [[[ nan,  nan,  nan, ...,  nan,  nan,  nan],\n",
      "         [ nan,  nan,  nan, ...,  nan,  nan,  nan],\n",
      "         [ nan,  nan,  nan, ...,  nan,  nan,  nan],\n",
      "         ..., \n",
      "         [ nan,  nan,  nan, ...,  nan,  nan,  nan],\n",
      "         [ nan,  nan,  nan, ...,  nan,  nan,  nan],\n",
      "         [ nan,  nan,  nan, ...,  nan,  nan,  nan]],\n",
      "\n",
      "        [[ nan,  nan,  nan, ...,  nan,  nan,  nan],\n",
      "         [ nan,  nan,  nan, ...,  nan,  nan,  nan],\n",
      "         [ nan,  nan,  nan, ...,  nan,  nan,  nan],\n",
      "         ..., \n",
      "         [ nan,  nan,  nan, ...,  nan,  nan,  nan],\n",
      "         [ nan,  nan,  nan, ...,  nan,  nan,  nan],\n",
      "         [ nan,  nan,  nan, ...,  nan,  nan,  nan]],\n",
      "\n",
      "        [[ nan,  nan,  nan, ...,  nan,  nan,  nan],\n",
      "         [ nan,  nan,  nan, ...,  nan,  nan,  nan],\n",
      "         [ nan,  nan,  nan, ...,  nan,  nan,  nan],\n",
      "         ..., \n",
      "         [ nan,  nan,  nan, ...,  nan,  nan,  nan],\n",
      "         [ nan,  nan,  nan, ...,  nan,  nan,  nan],\n",
      "         [ nan,  nan,  nan, ...,  nan,  nan,  nan]],\n",
      "\n",
      "        ..., \n",
      "        [[ nan,  nan,  nan, ...,  nan,  nan,  nan],\n",
      "         [ nan,  nan,  nan, ...,  nan,  nan,  nan],\n",
      "         [ nan,  nan,  nan, ...,  nan,  nan,  nan],\n",
      "         ..., \n",
      "         [ nan,  nan,  nan, ...,  nan,  nan,  nan],\n",
      "         [ nan,  nan,  nan, ...,  nan,  nan,  nan],\n",
      "         [ nan,  nan,  nan, ...,  nan,  nan,  nan]],\n",
      "\n",
      "        [[ nan,  nan,  nan, ...,  nan,  nan,  nan],\n",
      "         [ nan,  nan,  nan, ...,  nan,  nan,  nan],\n",
      "         [ nan,  nan,  nan, ...,  nan,  nan,  nan],\n",
      "         ..., \n",
      "         [ nan,  nan,  nan, ...,  nan,  nan,  nan],\n",
      "         [ nan,  nan,  nan, ...,  nan,  nan,  nan],\n",
      "         [ nan,  nan,  nan, ...,  nan,  nan,  nan]],\n",
      "\n",
      "        [[ nan,  nan,  nan, ...,  nan,  nan,  nan],\n",
      "         [ nan,  nan,  nan, ...,  nan,  nan,  nan],\n",
      "         [ nan,  nan,  nan, ...,  nan,  nan,  nan],\n",
      "         ..., \n",
      "         [ nan,  nan,  nan, ...,  nan,  nan,  nan],\n",
      "         [ nan,  nan,  nan, ...,  nan,  nan,  nan],\n",
      "         [ nan,  nan,  nan, ...,  nan,  nan,  nan]]]], dtype=float32)]\n",
      "[]\n",
      "[]\n",
      "[]\n"
     ]
    }
   ],
   "source": [
    "for l in model.layers:\n",
    "    print l.get_weights()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "[X_test, L_test, mask_test], Y_test = gen.next()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dtype('float32')"
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Y_test.dtype"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([6, 6, 6, 6, 6, 6, 0, 5, 6, 4, 6, 6, 4, 4, 3, 2, 6, 5, 6, 6, 0, 6, 1,\n",
       "       6, 6])"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Y_test.argmax(axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "predections = model.predict([X_test, L_test, mask_test])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7,\n",
       "       7, 7])"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predections.argmax(axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[  0.,   0.,   0.,   0.,   0.,   0.,   0.,  inf,  inf,  inf,  inf,\n",
       "         inf,  inf,  inf],\n",
       "       [  0.,   0.,   0.,   0.,   0.,   0.,   0.,  inf,  inf,  inf,  inf,\n",
       "         inf,  inf,  inf],\n",
       "       [  0.,   0.,   0.,   0.,   0.,   0.,   0.,  inf,  inf,  inf,  inf,\n",
       "         inf,  inf,  inf],\n",
       "       [  0.,   0.,   0.,   0.,   0.,   0.,   0.,  inf,  inf,  inf,  inf,\n",
       "         inf,  inf,  inf],\n",
       "       [  0.,   0.,   0.,   0.,   0.,   0.,   0.,  inf,  inf,  inf,  inf,\n",
       "         inf,  inf,  inf],\n",
       "       [  1.,   1.,   1.,   1.,   1.,   1.,   1.,  inf,  inf,  inf,  inf,\n",
       "         inf,  inf,  inf],\n",
       "       [  0.,   0.,   0.,   0.,   0.,   0.,   0.,  inf,  inf,  inf,  inf,\n",
       "         inf,  inf,  inf],\n",
       "       [  0.,   0.,   0.,   0.,   0.,   0.,   0.,  inf,  inf,  inf,  inf,\n",
       "         inf,  inf,  inf],\n",
       "       [  0.,   0.,   0.,   0.,   0.,   0.,   0.,  inf,  inf,  inf,  inf,\n",
       "         inf,  inf,  inf],\n",
       "       [  0.,   0.,   0.,   0.,   0.,   0.,   0.,  inf,  inf,  inf,  inf,\n",
       "         inf,  inf,  inf],\n",
       "       [  0.,   0.,   0.,   0.,   0.,   0.,   0.,  inf,  inf,  inf,  inf,\n",
       "         inf,  inf,  inf],\n",
       "       [  0.,   0.,   0.,   0.,   0.,   0.,   0.,  inf,  inf,  inf,  inf,\n",
       "         inf,  inf,  inf],\n",
       "       [  1.,   1.,   1.,   1.,   1.,   1.,   1.,  inf,  inf,  inf,  inf,\n",
       "         inf,  inf,  inf],\n",
       "       [  0.,   0.,   0.,   0.,   0.,   0.,   0.,  inf,  inf,  inf,  inf,\n",
       "         inf,  inf,  inf],\n",
       "       [  0.,   0.,   0.,   0.,   0.,   0.,   0.,  inf,  inf,  inf,  inf,\n",
       "         inf,  inf,  inf],\n",
       "       [  0.,   0.,   0.,   0.,   0.,   0.,   0.,  inf,  inf,  inf,  inf,\n",
       "         inf,  inf,  inf],\n",
       "       [  0.,   0.,   0.,   0.,   0.,   0.,   0.,  inf,  inf,  inf,  inf,\n",
       "         inf,  inf,  inf],\n",
       "       [  0.,   0.,   0.,   0.,   0.,   0.,   0.,  inf,  inf,  inf,  inf,\n",
       "         inf,  inf,  inf],\n",
       "       [  0.,   0.,   0.,   0.,   0.,   0.,   0.,  inf,  inf,  inf,  inf,\n",
       "         inf,  inf,  inf],\n",
       "       [  0.,   0.,   0.,   0.,   0.,   0.,   0.,  inf,  inf,  inf,  inf,\n",
       "         inf,  inf,  inf],\n",
       "       [  0.,   0.,   0.,   0.,   0.,   0.,   0.,  inf,  inf,  inf,  inf,\n",
       "         inf,  inf,  inf],\n",
       "       [  0.,   0.,   0.,   0.,   0.,   0.,   0.,  inf,  inf,  inf,  inf,\n",
       "         inf,  inf,  inf],\n",
       "       [  0.,   0.,   0.,   0.,   0.,   0.,   0.,  inf,  inf,  inf,  inf,\n",
       "         inf,  inf,  inf],\n",
       "       [  0.,   0.,   0.,   0.,   0.,   0.,   0.,  inf,  inf,  inf,  inf,\n",
       "         inf,  inf,  inf],\n",
       "       [  0.,   0.,   0.,   0.,   0.,   0.,   0.,  inf,  inf,  inf,  inf,\n",
       "         inf,  inf,  inf]], dtype=float32)"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predections"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
